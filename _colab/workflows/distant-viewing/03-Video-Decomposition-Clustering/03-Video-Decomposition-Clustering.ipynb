{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![video decomposition clustering](https://raw.githubusercontent.com/ARVEST-APP/ml-notebooks/refs/heads/main/docs/images/notebooks/video-decomposiiton-clustering.png)\n",
    "\n",
    "In this notebook, we shall start learning about the [Arvest](https://arvest.app/en) API and its [python package](https://github.com/ARVEST-APP/arvest-api) by taking some audiovisual IIIF Manifests sorted on Arvest, decomposing them into images, and using clustering and other machine learning techniques to create some interactive interfaces that we can upload and use in Arvest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "Let's begin by installing and importing all of the different components we will need.\n",
    "\n",
    "If you're currently in a workshop - hello! üëã Set the `IM_IN_A_WORKSHOP` so that setup is a bit quicker and to download all the data that result from heavier processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_IN_A_WORKSHOP = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get everything we need set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing and importing packages...\")\n",
    "\n",
    "# Install\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/ARVEST-APP/arvest-api.git > /dev/null 2>&1\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/ARVEST-APP/arvest-api-tools.git > /dev/null 2>&1\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/jdchart/jh-py-utils.git > /dev/null 2>&1\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/iiif-prezi/iiif-prezi3.git > /dev/null 2>&1\n",
    "!pip install -q --disable-pip-version-check mediapipe > /dev/null 2>&1\n",
    "!pip install -q --disable-pip-version-check opencv-python > /dev/null 2>&1\n",
    "!pip install -q --disable-pip-version-check scikit-learn > /dev/null 2>&1\n",
    "!pip install -q --disable-pip-version-check matplotlib > /dev/null 2>&1\n",
    "\n",
    "if not IM_IN_A_WORKSHOP:\n",
    "    !pip install -q --disable-pip-version-check git+https://github.com/distant-viewing/dvt.git\n",
    "    import dvt\n",
    "\n",
    "# Import packages\n",
    "import arvestapi\n",
    "import arvesttools.manifest_creation\n",
    "from jhutils.local_files import read_json, write_json, get_image_info, collect_files\n",
    "import jhutils.online_files\n",
    "from jhutils.misc import print_progress_bar_colab, slugify\n",
    "import os\n",
    "import iiif_prezi3\n",
    "import shutil\n",
    "import requests\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import mimetypes\n",
    "import mediapipe as mp\n",
    "\n",
    "DATA_DL_PATH = os.path.join(os.getcwd(), \"downloaded_data\")\n",
    "\n",
    "if IM_IN_A_WORKSHOP:\n",
    "    print(\"Downloading data...\")\n",
    "    os.makedirs(os.path.join(os.getcwd(), \"downloaded_data\"), exist_ok=True)\n",
    "    jhutils.online_files.download_zip(\"https://github.com/ARVEST-APP/arvest-workshops/raw/main/resources/workshop-notebook-resources.zip\", DATA_DL_PATH)\n",
    "    print(\"üëç Downloaded data\")\n",
    "\n",
    "print(\"üëç Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Arvest\n",
    "\n",
    "First, we need to \"connect\" to Arvest using the Arvest API package. For this, we need our user `email` and our `password` which we will give to an instance of the `arvestapi.Arvest()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's connect to our Arvest account:\n",
    "EMAIL = \"arvestuser@gmail.com\"\n",
    "PASSWORD = \"password\"\n",
    "\n",
    "ar = arvestapi.Arvest(EMAIL, PASSWORD)\n",
    "print(f\"üëç Succesfully connected to Arvest with \\\"{ar.profile.name}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get videos\n",
    "First we will need some sources to process! In this example, we shall be comparing different productions of Pina Bausch's [_Caf√© M√ºller_](https://en.wikipedia.org/wiki/Caf%C3%A9_M%C3%BCller).\n",
    "\n",
    "It so happens that we have some video Manifests on our Arvest account. Let's get our Manifests using the Arvest API's `get_manifests()` function.\n",
    "\n",
    "Once e have all of our Manifests, we can cycle through them, and check if each one is one of the Manifests that we're looking for. We know that our Manifest items have a specific description, so lets check that by inspecting the `description` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_MANIFESTS = []\n",
    "\n",
    "# Get all of our manifests:\n",
    "manifests = ar.get_manifests()\n",
    "\n",
    "# Get all of the manifests with a given description\n",
    "for manifest in manifests:\n",
    "    if manifest.description == \"Caf√© M√ºller video recording\":\n",
    "        VIDEO_MANIFESTS.append(manifest)\n",
    "\n",
    "print(f\"üîç Found {len(VIDEO_MANIFESTS)} manifests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the videos for processing\n",
    "\n",
    "In order to process the videos we will need to be able to access them locally, which means that we will need to download them into our session. We can do this by getting the Manifest content, and then retrieving the `id` of the video of the first canvas.\n",
    "\n",
    "‚ÑπÔ∏è This step will be skipped if you downloaded the workshop elements, however you cans till run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_VIDEO_PATHS = {}\n",
    "\n",
    "if not IM_IN_A_WORKSHOP:\n",
    "    for i, manifest in enumerate(VIDEO_MANIFESTS):\n",
    "        video_url = manifest.get_content()[\"items\"][0][\"items\"][0][\"items\"][0][\"body\"][\"id\"]\n",
    "        print_progress_bar_colab(i + 1, len(VIDEO_MANIFESTS), f\"(downloading {video_url})...\")\n",
    "\n",
    "        path = jhutils.online_files.download(video_url, dir = os.path.join(os.getcwd(), \"data\", \"videos\"))\n",
    "        LOCAL_VIDEO_PATHS[slugify(manifest.get_content()[\"id\"])] = {\"path\" : path, \"manifest_path\" : manifest.get_full_url()}\n",
    "else:\n",
    "    LOCAL_VIDEO_PATHS = read_json(os.path.join(DATA_DL_PATH, \"workshop-notebook-resources\", \"local_video_paths.json\"))\n",
    "\n",
    "print(f\"üëç Downloaded videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract images\n",
    "\n",
    "We're going to process one image for every second of video in order to get a good idea about the visual composition of each video. To do this we'll first need to extract images from the videos.\n",
    "\n",
    "‚ÑπÔ∏è This step will be skipped if you downloaded the workshop elements, however you cans till run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_IMAGE_PATHS = []\n",
    "INTERVAL = 2\n",
    "img_count = 0\n",
    "out_folder = os.path.join(os.getcwd(), \"data\", \"images\")\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "for i, video_name in enumerate(LOCAL_VIDEO_PATHS):\n",
    "    print_progress_bar_colab(i + 1, len(LOCAL_VIDEO_PATHS), f\"(treating {video_name})...\")\n",
    "\n",
    "    # Open video file with opencv 2 and get properties\n",
    "    cap = cv2.VideoCapture(LOCAL_VIDEO_PATHS[video_name][\"path\"])\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps\n",
    "\n",
    "    # For iteration:\n",
    "    frame_interval = int(fps * INTERVAL)\n",
    "    frame_num = 0\n",
    "    saved_frame_count = 0\n",
    "\n",
    "    while frame_num < total_frames:\n",
    "        # Get frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Save image\n",
    "        timestamp_secs = frame_num / fps\n",
    "        output_path = os.path.join(out_folder, f\"img_{img_count:04d}.jpg\")\n",
    "        cv2.imwrite(output_path, frame)\n",
    "        LOCAL_IMAGE_PATHS.append({\"img_path\" : output_path, \"index\" : img_count, \"frame\" : saved_frame_count, \"time\" : timestamp_secs, \"video_name\" : video_name})\n",
    "\n",
    "        img_count = img_count + 1\n",
    "\n",
    "        saved_frame_count += 1\n",
    "        frame_num += frame_interval\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "print(f\"üèûÔ∏è Finished extracting {img_count} images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Process embeddings\n",
    "Next we can use the [distant viewing toolkit](https://github.com/distant-viewing/dvt) to map the images within an embedding space. \n",
    "\n",
    "We'll save the embedding data as a **numpy file** (`.npy`) so that we don't have to run this step again.\n",
    "\n",
    "‚ÑπÔ∏è If you are running this for the first time dvt it will download the model onto your computer.\n",
    "\n",
    "‚ÑπÔ∏è This step will be skipped if you downloaded the workshop elements, however you cans till run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing...\")\n",
    "\n",
    "if not IM_IN_A_WORKSHOP:\n",
    "    # Instance of dvt AnnoEmbed class:\n",
    "    embedder = dvt.AnnoEmbed()\n",
    "\n",
    "    first_time = True\n",
    "\n",
    "    for i, image in enumerate(LOCAL_IMAGE_PATHS):\n",
    "        print_progress_bar_colab(i + 1, len(LOCAL_IMAGE_PATHS), f\"(treating {os.path.basename(image['img_path'])})...\")\n",
    "\n",
    "        image_as_np = cv2.imread(image['img_path'])\n",
    "        image_as_np = cv2.cvtColor(image_as_np, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        embedding = embedder.run(image_as_np)[\"embedding\"]\n",
    "        if first_time:\n",
    "            embedding_list = embedding\n",
    "            first_time = False\n",
    "        else:\n",
    "            embedding_list = np.vstack((embedding_list, embedding))\n",
    "\n",
    "    embedding_file = os.path.join(os.getcwd(), \"data\", \"embeddings.npy\")\n",
    "    np.save(embedding_file, embedding_list)\n",
    "\n",
    "print(\"üëç Finished collecting embeddings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dimensionality reduction and clustering\n",
    "Now that we have our embedding data, we can use dimensionality reduction to crunch all of these dimensions down into 2 so that they can be projected into a 2-dimensional space. \n",
    "\n",
    "To do this, we'll use the dimensionality reduction algorithm [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). \n",
    "\n",
    "Note that we also do some pre- and post-processing, the full process is: standardisation -> dimensionality reduction -> normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IM_IN_A_WORKSHOP:\n",
    "    embedding_file = os.path.join(os.getcwd(), \"data\", \"embeddings.npy\")\n",
    "else:\n",
    "    embedding_file = os.path.join(DATA_DL_PATH, \"workshop-notebook-resources\", \"embeddings.npy\")\n",
    "    \n",
    "embedding_list = np.load(embedding_file)\n",
    "\n",
    "standardized = StandardScaler().fit_transform(embedding_list)\n",
    "\n",
    "tsne = TSNE(n_components = 2, perplexity = 50, learning_rate=200, n_iter=5000)\n",
    "reduced = tsne.fit_transform(standardized)\n",
    "\n",
    "normalized = MinMaxScaler((0, 1)).fit_transform(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we like, we can visualize the data in a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed = np.transpose(normalized)\n",
    "plt.scatter(transposed[0], transposed[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (optional)\n",
    "\n",
    "Next we can perform some clustering on this data using [K-Means](https://en.wikipedia.org/wiki/K-means_clustering).\n",
    "\n",
    "This will allow us to create subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 6\n",
    "\n",
    "kmeans = KMeans(n_clusters = NUM_CLUSTERS, random_state = 0, n_init = \"auto\")\n",
    "clusters = kmeans.fit(normalized).labels_\n",
    "\n",
    "# Create a random colour map for visualisation:\n",
    "colour_map = {}\n",
    "used = []\n",
    "for item in clusters:\n",
    "    if item not in used:\n",
    "        colour_map[str(item)] = (random.random(), random.random(), random.random())\n",
    "        used.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the clusters in a scatter plot like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed = np.transpose(normalized)\n",
    "col = []\n",
    "for item in clusters:\n",
    "    col.append(colour_map[str(item)])\n",
    "\n",
    "plt.scatter(transposed[0], transposed[1], c = col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Export to Arvest\n",
    "Finally, we shall export the results of our analysis to an image file, and create an annotated (and therefore interactive) IIIF Manifest that can be consulted in [Arvest](https://arvest.app/en). \n",
    "\n",
    "First, we shall create a high-res PNG file that projects the corresponding images into the 2D space of the dimensionality reduction. \n",
    "\n",
    "We shall also keep a track of the coordinates so that we can create our annotations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = os.path.join(os.getcwd(), \"data\", \"visualization-image.png\")\n",
    "COORDINATES_PATH = os.path.join(os.getcwd(), \"data\", \"visualization-coordinates.json\")\n",
    "\n",
    "WIDTH = 6000\n",
    "HEIGHT = 6000\n",
    "PADDING = 100\n",
    "IMAGE_ZOOM = 0.08\n",
    "\n",
    "def scale(val, old_min, old_max, new_min, new_max):\n",
    "    return new_min + (((val - old_min) * (new_max - new_min)) / (old_max - old_min))\n",
    "\n",
    "# Function for adding each image to the main image:\n",
    "def add_image(full_image, coordinates_list, img_path, coordinates, manifest, time):\n",
    "  img_data = get_image_info(img_path)\n",
    "  this_img = Image.open(img_path)\n",
    "\n",
    "  w = int(img_data[\"width\"] * IMAGE_ZOOM)\n",
    "  h = int(img_data[\"height\"] * IMAGE_ZOOM)\n",
    "  x = int(scale(int(int(float(coordinates[0]) * WIDTH) - (w * 0.5)), 0, WIDTH, PADDING, WIDTH - (PADDING * 2)))\n",
    "  y = int(scale(int(int(float(coordinates[1]) * HEIGHT) - (h * 0.5)), 0, HEIGHT, PADDING, HEIGHT - (PADDING * 2)))\n",
    "\n",
    "  this_img = this_img.resize((w, h))\n",
    "  full_image.paste(this_img, (x, y))\n",
    "\n",
    "  coordinates_list.append({\"url\" : manifest, \"x\" : x, \"y\" : y, \"w\" : w, \"h\" : h, \"img_path\" : os.path.basename(img_path), \"time\" : time})\n",
    "\n",
    "# Initialize image and coordinates\n",
    "full_image = Image.new('RGBA', (WIDTH, HEIGHT))\n",
    "coordinates = {\"images\" : []}\n",
    "\n",
    "# Add all of the images:\n",
    "for i, item in enumerate(normalized):\n",
    "  image_data = LOCAL_IMAGE_PATHS[i]\n",
    "  print_progress_bar_colab(i + 1, len(LOCAL_IMAGE_PATHS), f\"Treating {image_data['img_path']}\")\n",
    "\n",
    "  add_image(full_image, coordinates[\"images\"], image_data['img_path'], item, LOCAL_VIDEO_PATHS[image_data['video_name']][\"manifest_path\"], image_data['time'])\n",
    "\n",
    "full_image.save(IMAGE_PATH)\n",
    "write_json(COORDINATES_PATH, coordinates)\n",
    "\n",
    "print(\"üé® Image created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload image to Arvest\n",
    "Let's use the `add_media()` function to upload a local image to Arvest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_media = ar.add_media(path = IMAGE_PATH)\n",
    "added_media.update_title(\"Pina Bausch recordings projection\")\n",
    "added_media.update_description(\"A projection in 2D space of Pina Bausch recordings.\")\n",
    "\n",
    "print(f\"üëç Media uploaded to Arvest at the following url: {added_media.get_full_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and upload Manifest\n",
    "\n",
    "Now let's create the Manifest. We'll use the [arvestapitools](https://github.com/ARVEST-APP/arvest-api-tools) `media_to_manifest()` utility function, which can take an Arvest media item and create a Manifest from it.\n",
    "\n",
    "Once we've created the Manifest, we can add annotations to render it interactive. We'll add an annotation for each of the images with the `add_textual_annotation()` function with the corresponding video Manifest url and spatial position and dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Manifest:\n",
    "manifest = arvesttools.manifest_creation.media_to_manifest(added_media)\n",
    "\n",
    "# Add an annotation for each Manifest:\n",
    "for item in coordinates[\"images\"]:\n",
    "    manifest_url = item[\"url\"]\n",
    "    xywh = {\"x\" : item[\"x\"], \"y\" : item[\"y\"], \"w\" : item[\"w\"], \"h\" : item[\"h\"]}\n",
    "    \n",
    "    arvesttools.manifest_creation.add_textual_annotation(\n",
    "        manifest,\n",
    "        text_content = f\"<p><strong>{item['img_path']}</strong><br><i>{item['time']} seconds</i></p>\",\n",
    "        xywh = xywh,\n",
    "        linked_manifest = manifest_url\n",
    "    )\n",
    "\n",
    "# Save to disk:\n",
    "local_path = os.path.join(os.getcwd(), \"data\", \"projection-manifest.json\")\n",
    "write_json(local_path, manifest.dict())\n",
    "\n",
    "# And upload to Arvest:\n",
    "added_manifest = ar.add_manifest(path = local_path, update_id = True)\n",
    "added_manifest.update_title(\"Image embedding projection\")\n",
    "added_manifest.update_description(\"Projection of a collection of images in 2-D space.\")\n",
    "added_manifest.update_thumbnail_url(added_media.thumbnail_url)\n",
    "\n",
    "print(f\"üëç Manifest uploaded to Arvest at the following url: {added_manifest.get_preview_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pose detection\n",
    "Now that we've made this initial interface, we can see that the image embeddings as a feature set mainly produce a projection that groups images together by recording. We need to rethink what it is that we are trying to find in our data.\n",
    "\n",
    "In the context of dance analysis, it could be interesting to project images ccording to post detection data. In this first cell we set up some utility functions for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose(static_image_mode=True)\n",
    "\n",
    "def normalize_landmarks(landmarks):\n",
    "    # Get key landmarks\n",
    "    left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "    right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "    left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "\n",
    "    # Calculate hip center\n",
    "    hip_center = np.array([(left_hip.x + right_hip.x) / 2,\n",
    "                           (left_hip.y + right_hip.y) / 2,\n",
    "                           (left_hip.z + right_hip.z) / 2])\n",
    "\n",
    "    # Translate landmarks to hip center\n",
    "    translated = []\n",
    "    for lm in landmarks:\n",
    "        translated.append(np.array([lm.x, lm.y, lm.z]) - hip_center)\n",
    "\n",
    "    # Calculate torso length\n",
    "    shoulder_center = (np.array([left_shoulder.x, left_shoulder.y, left_shoulder.z]) +\n",
    "                       np.array([right_shoulder.x, right_shoulder.y, right_shoulder.z])) / 2\n",
    "    torso_length = np.linalg.norm(shoulder_center - hip_center)\n",
    "\n",
    "    # Scale landmarks\n",
    "    normalized = [coords / torso_length for coords in translated]\n",
    "\n",
    "    return normalized\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    v1 = v1 / np.linalg.norm(v1)\n",
    "    v2 = v2 / np.linalg.norm(v2)\n",
    "    dot = np.clip(np.dot(v1, v2), -1.0, 1.0)\n",
    "    return np.arccos(dot)\n",
    "\n",
    "def compute_joint_angles(landmarks):\n",
    "    def get_point(idx): return np.array([landmarks[idx].x, landmarks[idx].y, landmarks[idx].z])\n",
    "\n",
    "    angles = []\n",
    "\n",
    "    # Left elbow\n",
    "    shoulder = get_point(mp_pose.PoseLandmark.LEFT_SHOULDER.value)\n",
    "    elbow = get_point(mp_pose.PoseLandmark.LEFT_ELBOW.value)\n",
    "    wrist = get_point(mp_pose.PoseLandmark.LEFT_WRIST.value)\n",
    "    angles.append(angle_between(shoulder - elbow, wrist - elbow))\n",
    "\n",
    "    # Right elbow\n",
    "    shoulder = get_point(mp_pose.PoseLandmark.RIGHT_SHOULDER.value)\n",
    "    elbow = get_point(mp_pose.PoseLandmark.RIGHT_ELBOW.value)\n",
    "    wrist = get_point(mp_pose.PoseLandmark.RIGHT_WRIST.value)\n",
    "    angles.append(angle_between(shoulder - elbow, wrist - elbow))\n",
    "\n",
    "    # Add more joints as needed\n",
    "\n",
    "    return angles\n",
    "\n",
    "def flatten_visible_keypoints(landmarks, visibility_threshold=0.5):\n",
    "    keypoints = []\n",
    "    for lm in landmarks:\n",
    "        if lm.visibility > visibility_threshold:\n",
    "            keypoints.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "        else:\n",
    "            # Optional: fill with zeros or skip\n",
    "            keypoints.extend([0, 0, 0, 0])\n",
    "    return keypoints\n",
    "\n",
    "def draw_pose_on_transparent(image_shape, landmarks):\n",
    "    height, width = image_shape[:2]\n",
    "\n",
    "    # Step 1: Draw pose on white background (3-channel BGR)\n",
    "    white_bg = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    mp_drawing.draw_landmarks(\n",
    "        white_bg,\n",
    "        landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(0, 0, 0), thickness=2, circle_radius=2),\n",
    "        mp_drawing.DrawingSpec(color=(0, 0, 0), thickness=2)\n",
    "    )\n",
    "\n",
    "    # Step 2: Convert white to transparent\n",
    "    white_pixels = np.all(white_bg == [255, 255, 255], axis=-1)\n",
    "    alpha_channel = np.where(white_pixels, 0, 255).astype(np.uint8)\n",
    "\n",
    "    rgba = cv2.cvtColor(white_bg, cv2.COLOR_BGR2BGRA)\n",
    "    rgba[..., 3] = alpha_channel\n",
    "\n",
    "    return rgba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the pose detection analysis. We follow these steps:\n",
    "\n",
    "- perform analysis\n",
    "- normalize poses (remove translation and scale differences between poses by using the hip center as an origin and torso length or scaling)\n",
    "- Use angles between joins as features\n",
    "- make sure that we're only treating poses where a certain amount of keypoints were visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_TO_ANALYSE = collect_files(os.path.join(os.getcwd(), \"data\", \"images\"), [\"jpg\"])\n",
    "OUTPUT_FULL_IMAGE = False\n",
    "OUTPUT_FOLDER = os.path.join(os.getcwd(), \"data\", \"images\", \"pose_detection\")\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok = True)\n",
    "\n",
    "# Step 1: Extract pose features\n",
    "filenames = []\n",
    "features = []\n",
    "images = []\n",
    "landmarks_list = []\n",
    "\n",
    "for i, filename in enumerate(IMAGES_TO_ANALYSE):\n",
    "    print_progress_bar_colab(i + 1, len(IMAGES_TO_ANALYSE), f\"(treating {os.path.basename(filename)})\")\n",
    "\n",
    "    image_path = filename\n",
    "\n",
    "    filename = os.path.basename(filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        normalized_landmarks = normalize_landmarks(landmarks)\n",
    "        angles = compute_joint_angles(landmarks)\n",
    "        flattened = flatten_visible_keypoints(landmarks)\n",
    "\n",
    "        features.append(flattened)\n",
    "        images.append(image)\n",
    "        landmarks_list.append(results.pose_landmarks)\n",
    "        filenames.append(os.path.basename(filename))\n",
    "\n",
    "        if OUTPUT_FULL_IMAGE:\n",
    "            output_image = image.copy()\n",
    "            mp_drawing.draw_landmarks(\n",
    "                output_image,\n",
    "                results.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "            )\n",
    "            cv2.imwrite(os.path.join(OUTPUT_FOLDER, f\"{os.path.splitext(os.path.basename(filename))[0]}.png\"), output_image)\n",
    "        else:\n",
    "            pose_image = draw_pose_on_transparent(image.shape, results.pose_landmarks)\n",
    "            cv2.imwrite(os.path.join(OUTPUT_FOLDER, f\"{os.path.splitext(os.path.basename(filename))[0]}.png\"), pose_image)\n",
    "\n",
    "pose.close()\n",
    "features_array = np.array(features)\n",
    "\n",
    "print(f\"üëç Finished! Final corpus is comprised of {len(features_array)} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized = StandardScaler().fit_transform(features_array)\n",
    "\n",
    "tsne = TSNE(n_components = 2, perplexity = 50, learning_rate=200, n_iter=5000)\n",
    "reduced = tsne.fit_transform(standardized)\n",
    "\n",
    "normalized = MinMaxScaler((0, 1)).fit_transform(reduced)\n",
    "\n",
    "transposed = np.transpose(normalized)\n",
    "plt.scatter(transposed[0], transposed[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, like before, we can create an intractive visualization. Let's start by making the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = os.path.join(os.getcwd(), \"data\", \"pose-detection-visualization-image.png\")\n",
    "COORDINATES_PATH = os.path.join(os.getcwd(), \"data\", \"pose-detection-visualization-coordinates.json\")\n",
    "\n",
    "WIDTH = 6000\n",
    "HEIGHT = 6000\n",
    "PADDING = 100\n",
    "IMAGE_ZOOM = 0.2\n",
    "\n",
    "def scale(val, old_min, old_max, new_min, new_max):\n",
    "    return new_min + (((val - old_min) * (new_max - new_min)) / (old_max - old_min))\n",
    "\n",
    "# Function for adding each image to the main image:\n",
    "def add_image(full_image, coordinates_list, img_path, coordinates, manifest, time):\n",
    "  img_data = get_image_info(img_path)\n",
    "  this_img = Image.open(img_path).convert(\"RGBA\")  # Ensure RGBA\n",
    "\n",
    "  w = int(img_data[\"width\"] * IMAGE_ZOOM)\n",
    "  h = int(img_data[\"height\"] * IMAGE_ZOOM)\n",
    "  x = int(scale(int(int(float(coordinates[0]) * WIDTH) - (w * 0.5)), 0, WIDTH, PADDING, WIDTH - (PADDING * 2)))\n",
    "  y = int(scale(int(int(float(coordinates[1]) * HEIGHT) - (h * 0.5)), 0, HEIGHT, PADDING, HEIGHT - (PADDING * 2)))\n",
    "\n",
    "  this_img = this_img.resize((w, h), resample=Image.LANCZOS)\n",
    "\n",
    "  # Create a transparent image same size as full_image\n",
    "  temp_img = Image.new(\"RGBA\", full_image.size, (0, 0, 0, 0))\n",
    "  temp_img.paste(this_img, (x, y), mask=this_img)  # Use self as mask for alpha paste\n",
    "\n",
    "  # Composite with existing image\n",
    "  full_image.alpha_composite(temp_img)\n",
    "\n",
    "  coordinates_list.append({\"url\": manifest,\"x\": x,\"y\": y,\"w\": w,\"h\": h,\"img_path\": os.path.basename(img_path),\"time\": time})\n",
    "\n",
    "# Initialize image and coordinates\n",
    "full_image = Image.new('RGBA', (WIDTH, HEIGHT))\n",
    "coordinates = {\"images\" : []}\n",
    "\n",
    "pose_images = collect_files(OUTPUT_FOLDER, [\"png\"])\n",
    "\n",
    "# Add all of the images:\n",
    "for i, pose_image_path in enumerate(pose_images):\n",
    "   print_progress_bar_colab(i + 1, len(pose_images), f\"Treating {pose_image_path}\")\n",
    "\n",
    "   real_index = int(pose_image_path.split(f\"{OUTPUT_FOLDER}/img_\")[1].split(\".png\")[0])\n",
    "   image_data = LOCAL_IMAGE_PATHS[real_index]\n",
    "   item = normalized[i]\n",
    "\n",
    "   add_image(full_image, coordinates[\"images\"], pose_image_path, item, LOCAL_VIDEO_PATHS[image_data['video_name']][\"manifest_path\"], image_data['time'])\n",
    "\n",
    "full_image.save(IMAGE_PATH)\n",
    "write_json(COORDINATES_PATH, coordinates)\n",
    "\n",
    "print(\"üé® Image created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can upload the image in the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_media = ar.add_media(path = IMAGE_PATH)\n",
    "added_media.update_title(\"Pina Bausch pose detection projection\")\n",
    "added_media.update_description(\"A projection in 2D space of Pina Bausch pose detections.\")\n",
    "\n",
    "print(f\"üëç Media uploaded to Arvest at the following url: {added_media.get_full_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally create the Manifest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Manifest:\n",
    "manifest = arvesttools.manifest_creation.media_to_manifest(added_media)\n",
    "\n",
    "# Add an annotation for each Manifest:\n",
    "for item in coordinates[\"images\"]:\n",
    "    manifest_url = item[\"url\"]\n",
    "    xywh = {\"x\" : item[\"x\"], \"y\" : item[\"y\"], \"w\" : item[\"w\"], \"h\" : item[\"h\"]}\n",
    "    \n",
    "    arvesttools.manifest_creation.add_textual_annotation(\n",
    "        manifest,\n",
    "        text_content = f\"<p><strong>{item['img_path']}</strong><br><i>{item['time']} seconds</i></p>\",\n",
    "        xywh = xywh,\n",
    "        linked_manifest = manifest_url\n",
    "    )\n",
    "\n",
    "# Save to disk:\n",
    "local_path = os.path.join(os.getcwd(), \"data\", \"projection-manifest.json\")\n",
    "write_json(local_path, manifest.dict())\n",
    "\n",
    "# And upload to Arvest:\n",
    "added_manifest = ar.add_manifest(path = local_path, update_id = True)\n",
    "added_manifest.update_title(\"Pose detection projection\")\n",
    "added_manifest.update_description(\"Projection of a collection of pose detections in 2-D space.\")\n",
    "added_manifest.update_thumbnail_url(added_media.thumbnail_url)\n",
    "\n",
    "print(f\"üëç Manifest uploaded to Arvest at the following url: {added_manifest.get_preview_url()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
