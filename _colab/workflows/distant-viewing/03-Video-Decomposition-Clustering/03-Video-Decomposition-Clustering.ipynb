{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![video decomposition clustering](https://raw.githubusercontent.com/ARVEST-APP/ml-notebooks/refs/heads/main/docs/images/notebooks/video-decomposiiton-clustering.png)\n",
    "\n",
    "In this notebook, we shall start learning about the [Arvest](https://arvest.app/en) API and its [python package](https://github.com/ARVEST-APP/arvest-api) by taking some audiovisual IIIF Manifests sorted on Arvest, decomposing them into images, and using clustering and other machine learning techniques to create some interactive interfaces that we can upload and use in Arvest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "Let's begin by installing and importing all of the different components we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing and importing packages...\n",
      "üëç Ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"Installing and importing packages...\")\n",
    "\n",
    "# Uninstall and reinstall packages for a clean environment\n",
    "!pip uninstall -q -y arvestapi\n",
    "!pip uninstall -q -y arvesttools\n",
    "!pip uninstall -q -y jhutils\n",
    "!pip uninstall -q -y iiif_prezi3\n",
    "!pip uninstall -q -y dvt\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/ARVEST-APP/arvest-api.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/ARVEST-APP/arvest-api-tools.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/jdchart/jh-py-utils.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/iiif-prezi/iiif-prezi3.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/distant-viewing/dvt.git\n",
    "!pip install -q --disable-pip-version-check opencv-python\n",
    "!pip install -q --disable-pip-version-check scikit-learn\n",
    "!pip install -q --disable-pip-version-check matplotlib\n",
    "\n",
    "# Import packages\n",
    "import arvestapi\n",
    "import arvesttools.manifest_creation\n",
    "from jhutils.local_files import read_json, write_json, get_image_info, collect_files\n",
    "import jhutils.online_files\n",
    "from jhutils.misc import print_progress_bar_colab, slugify\n",
    "import os\n",
    "import dvt\n",
    "import iiif_prezi3\n",
    "import shutil\n",
    "import requests\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import mimetypes\n",
    "\n",
    "print(\"üëç Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you're following a workshop session...\n",
    "\n",
    "If you're currently in a workshop - hello! üëã\n",
    "\n",
    "You can run this cell to download all of the data into your colab session so that you don't have to run all of the time-consuming processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_IN_A_WORKSHOP = False\n",
    "\n",
    "if IM_IN_A_WORKSHOP:\n",
    "    print(\"Downloading data...\")\n",
    "    \n",
    "    # Download and unpack zip:\n",
    "    os.makedirs(os.path.join(os.getcwd(), \"downloaded_data\"), exist_ok=True)\n",
    "    jhutils.online_files.download_zip(\"https://github.com/ARVEST-APP/arvest-workshops/raw/main/resources/workshop-dl-media.zip\", os.path.join(os.getcwd(), \"downloaded_data\"))\n",
    "    print(\"üëç Downloaded data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Arvest\n",
    "\n",
    "First, we need to \"connect\" to Arvest using the Arvest API package. For this, we need our user email and our password which we will give to an instance of the `arvestapi.Arvest()` class. For convenience, we've saved ours in a file which is why we get `LOGIN_DATA` by reading a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's connect to our Arvest account:\n",
    "LOGIN_DATA = os.path.join(os.getcwd(), \"login_private.json\")\n",
    "credentials = read_json(LOGIN_DATA)\n",
    "\n",
    "ar = arvestapi.Arvest(credentials[\"email\"], credentials[\"password\"])\n",
    "print(f\"üëç Succesfully connected to Arvest with \\\"{ar.profile.name}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get videos\n",
    "First we will need some sources to process! In this example, we shall be comparing different productions of Pina Bausch's [_Caf√© M√ºller_](https://en.wikipedia.org/wiki/Caf%C3%A9_M%C3%BCller).\n",
    "\n",
    "It so happens that we have some video Manifests on our Arvest account. Let's get our Manifests using the Arvest API's `get_manifests()` function.\n",
    "\n",
    "Once e have all of our Manifests, we can cycle through them, and check if each one is one of the Manifests that we're looking for. We know that our Manifest items have a specific description, so lets check that by inspecting the `description` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_MANIFESTS = []\n",
    "\n",
    "# Get all of our manifests:\n",
    "manifests = ar.get_manifests()\n",
    "\n",
    "# Get all of the manifests with a given description\n",
    "for manifest in manifests:\n",
    "    if manifest.description == \"Caf√© M√ºller video recording\":\n",
    "        VIDEO_MANIFESTS.append(manifest)\n",
    "\n",
    "print(f\"üîç Found {len(VIDEO_MANIFESTS)} manifests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the videos for processing\n",
    "\n",
    "In order to process the videos we will need to be able to access them locally, which means that we will need to download them into our session. We can do this by getting the Manifest content, and then retrieving the `id` of the video of the first canvas.\n",
    "\n",
    "‚ÑπÔ∏è This step will be skipped if you downloaded the workshop elements, however you cans till run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_VIDEO_PATHS = {}\n",
    "\n",
    "if not IM_IN_A_WORKSHOP:\n",
    "    for i, manifest in enumerate(VIDEO_MANIFESTS):\n",
    "        video_url = manifest.get_content()[\"items\"][0][\"items\"][0][\"items\"][0][\"body\"][\"id\"]\n",
    "        print_progress_bar(i + 1, len(VIDEO_MANIFESTS), f\"(downloading {video_url})...\")\n",
    "\n",
    "        path = jhutils.online_files.download(video_url, dir = os.path.join(os.getcwd(), \"data\", \"videos\"))\n",
    "        LOCAL_VIDEO_PATHS[slugify(manifest.get_content()[\"id\"])] = path\n",
    "else:\n",
    "    LOCAL_VIDEO_PATHS = {} # ADD THIS\n",
    "\n",
    "print(f\"üëç Downloaded videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract images\n",
    "\n",
    "We're going to process one image for every second of video in order to get a good idea about the visual composition of each video. To do this we'll first need to extract images from the videos.\n",
    "\n",
    "‚ÑπÔ∏è This step will be skipped if you downloaded the workshop elements, however you cans till run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_IMAGE_PATHS = {}\n",
    "INTERVAL = 1\n",
    "img_count = 0\n",
    "\n",
    "if not IM_IN_A_WORKSHOP:\n",
    "    for i, video_name in enumerate(LOCAL_VIDEO_PATHS):\n",
    "        print_progress_bar_colab(i + 1, len(LOCAL_VIDEO_PATHS), f\"(treating {video_name})...\")\n",
    "\n",
    "        # Output folder:\n",
    "        out_folder = os.path.join(os.getcwd(), \"data\", \"images\", video_name)\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "        LOCAL_IMAGE_PATHS[video_name] = out_folder\n",
    "\n",
    "        # Open video file with opencv 2 and get properties\n",
    "        cap = cv2.VideoCapture(LOCAL_VIDEO_PATHS[video_name])\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = total_frames / fps\n",
    "\n",
    "        # For iteration:\n",
    "        frame_interval = int(fps * INTERVAL)\n",
    "        frame_num = 0\n",
    "        saved_frame_count = 0\n",
    "\n",
    "        while frame_num < total_frames:\n",
    "            # Get frame\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Save image\n",
    "            timestamp_secs = frame_num / fps\n",
    "            output_path = os.path.join(out_folder, f\"frame_{saved_frame_count:04d}_{timestamp_secs:.2f}.jpg\")\n",
    "            cv2.imwrite(output_path, frame)\n",
    "            img_count = img_count + 1\n",
    "\n",
    "            saved_frame_count += 1\n",
    "            frame_num += frame_interval\n",
    "\n",
    "        cap.release()\n",
    "else:\n",
    "    LOCAL_IMAGE_PATHS = {} # ADD THIS\n",
    "\n",
    "print(f\"üèûÔ∏è Finished extracting {img_count} images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Process embeddings\n",
    "Next we can use the [distant viewing toolkit](https://github.com/distant-viewing/dvt) to map the images within an embedding space. \n",
    "\n",
    "We'll save the embedding data as a **numpy file** (`.npy`) so that we don't have to run this step again.\n",
    "\n",
    "‚ÑπÔ∏è If you are running this for the first time dvt it will download the model onto your computer.\n",
    "‚ÑπÔ∏è This step will be skipped if you downloaded the workshop elements, however you cans till run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing...\")\n",
    "\n",
    "# Instance of dvt AnnoEmbed class:\n",
    "embedder = dvt.AnnoEmbed()\n",
    "\n",
    "if not IM_IN_A_WORKSHOP:\n",
    "    first_time = True\n",
    "    for i, video_name in enumerate(LOCAL_IMAGE_PATHS):\n",
    "\n",
    "        corpus = collect_files(LOCAL_IMAGE_PATHS[video_name], [\"jpg\"])\n",
    "\n",
    "        for i, image_file in enumerate(corpus):\n",
    "            print_progress_bar_colab(i + 1, len(corpus), f\"Treating {os.path.basename(image_file)}\")\n",
    "\n",
    "            image_as_np = cv2.imread(image_file)\n",
    "            image_as_np = cv2.cvtColor(image_as_np, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            embedding = embedder.run(image_as_np)[\"embedding\"]\n",
    "            if first_time:\n",
    "                embedding_list = embedding\n",
    "                first_time = False\n",
    "            else:\n",
    "                embedding_list = np.vstack((embedding_list, embedding))\n",
    "\n",
    "    embedding_file = os.path.join(os.getcwd(), \"data\", \"embeddings.npy\")\n",
    "    np.save(embedding_file, embedding_list)\n",
    "\n",
    "print(\"üëçFinished collecting embeddings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dimensionality reduction and clustering\n",
    "Now that we have our embedding data, we can use dimensionality reduction to crunch all of these dimensions down into 2 so that they can be projected into a 2-dimensional space. \n",
    "\n",
    "To do this, we'll use the dimensionality reduction algorithm [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). \n",
    "\n",
    "Note that we also do some pre- and post-processing, the full process is: standardisation -> dimensionality reduction -> normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if not IM_IN_A_WORKSHOP:\n",
    "    embedding_file = os.path.join(os.getcwd(), \"data\", \"embeddings.npy\")\n",
    "else:\n",
    "    embedding_file = \"\" # ADD THIS\n",
    "    \n",
    "embedding_list = np.load(embedding_file)\n",
    "\n",
    "standardized = StandardScaler().fit_transform(embedding_list)\n",
    "\n",
    "tsne = TSNE(n_components = 2, perplexity = 5, learning_rate=10, n_iter=250)\n",
    "reduced = tsne.fit_transform(standardized)\n",
    "\n",
    "normalized = MinMaxScaler((0, 1)).fit_transform(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we like, we can visualize the data in a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed = np.transpose(normalized)\n",
    "plt.scatter(transposed[0], transposed[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (optional)\n",
    "\n",
    "Next we can perform some clustering on this data using [K-Means](https://en.wikipedia.org/wiki/K-means_clustering).\n",
    "\n",
    "This will allow us to create subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 6\n",
    "\n",
    "kmeans = KMeans(n_clusters = NUM_CLUSTERS, random_state = 0, n_init = \"auto\")\n",
    "clusters = kmeans.fit(normalized).labels_\n",
    "\n",
    "# Create a random colour map for visualisation:\n",
    "colour_map = {}\n",
    "used = []\n",
    "for item in clusters:\n",
    "    if item not in used:\n",
    "        colour_map[str(item)] = (random.random(), random.random(), random.random())\n",
    "        used.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the clusters in a scatter plot like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed = np.transpose(normalized)\n",
    "col = []\n",
    "for item in clusters:\n",
    "    col.append(colour_map[str(item)])\n",
    "\n",
    "plt.scatter(transposed[0], transposed[1], c = col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Export to Arvest\n",
    "Finally, we shall export the results of our analysis to an image file, and create an annotated (and therefore interactive) IIIF Manifest that can be consulted in [Arvest](https://arvest.app/en). First, we shall create a high-res PNG file that projects the corresponding images into the 2D space of the dimensionality reduction. We shall also keep a track of the coordinates so that we can create our annotations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = os.path.join(os.getcwd(), \"visualization-image.png\")\n",
    "COORDINATES_PATH = os.path.join(os.getcwd(), \"visualization-coordinates.json\")\n",
    "\n",
    "WIDTH = 5000\n",
    "HEIGHT = 5000\n",
    "PADDING = 100\n",
    "IMAGE_ZOOM = 0.1\n",
    "\n",
    "def scale(val, old_min, old_max, new_min, new_max):\n",
    "    return new_min + (((val - old_min) * (new_max - new_min)) / (old_max - old_min))\n",
    "\n",
    "# Function for adding each image to the main image:\n",
    "def add_image(full_image, coordinates_list, image_url, coordinates):\n",
    "  img_path = os.path.join(TEMP_FOLDER, os.path.basename(image_url))\n",
    "  img_data = get_image_info(img_path)\n",
    "  this_img = Image.open(img_path)\n",
    "\n",
    "  w = int(img_data[\"width\"] * IMAGE_ZOOM)\n",
    "  h = int(img_data[\"height\"] * IMAGE_ZOOM)\n",
    "  x = int(scale(int(int(float(coordinates[0]) * WIDTH) - (w * 0.5)), 0, WIDTH, PADDING, WIDTH - (PADDING * 2)))\n",
    "  y = int(scale(int(int(float(coordinates[1]) * HEIGHT) - (h * 0.5)), 0, HEIGHT, PADDING, HEIGHT - (PADDING * 2)))\n",
    "\n",
    "  this_img = this_img.resize((w, h))\n",
    "  full_image.paste(this_img, (x, y))\n",
    "\n",
    "  coordinates_list.append({\"url\" : image_url, \"x\" : x, \"y\" : y, \"w\" : w, \"h\" : h})\n",
    "\n",
    "# Initialize image and coordinates\n",
    "full_image = Image.new('RGBA', (WIDTH, HEIGHT))\n",
    "coordinates = {\"images\" : []}\n",
    "\n",
    "# Add all of the images:\n",
    "for i, item in enumerate(normalized):\n",
    "  image_data = corpus[i]\n",
    "  print_progress_bar_colab(i, len(corpus) - 1, f\"Treating {os.path.basename(image_data['url'])}\")\n",
    "  add_image(full_image, coordinates[\"images\"], image_data['url'], item)\n",
    "\n",
    "full_image.save(IMAGE_PATH)\n",
    "write_json(COORDINATES_PATH, coordinates)\n",
    "\n",
    "print(\"üé® Image created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Manifests\n",
    "Now we need to create our Manifests. In order to make the main visualization Manifest truly interactive, we shall also make a _Manifest for each of the images in our corpus_. This must be done first, as we will need the URLs of these Manifests when creating our annotations.\n",
    "\n",
    "First, we need to \"connect\" to Arvest using the Arvest API package. For this, we need our user email and our password which we will give to an instance of the `arvestapi.Arvest()` class. For convenience, we've saved ours in a file which is why we get `LOGIN_DATA` by reading a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's connect to our Arvest account:\n",
    "LOGIN_DATA = os.path.join(os.getcwd(), \"login_private.json\")\n",
    "credentials = read_json(LOGIN_DATA)\n",
    "\n",
    "ar = arvestapi.Arvest(credentials[\"email\"], credentials[\"password\"])\n",
    "print(f\"üëç Succesfully connected to Arvest with \\\"{ar.profile.name}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our Manifests using the [arvesttools](https://github.com/ARVEST-APP/arvest-api-tools) package's helper function `media_to_manifest()`. We'll create a Manifest for each file in our corpus, and keep a track of the URLs which are created in the `MANIFEST_DICT` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANIFEST_DICT = {}\n",
    "\n",
    "for i, image_data in enumerate(corpus):\n",
    "  print_progress_bar_colab(i + 1, len(corpus), f\"Creating a Manifest for {os.path.basename(image_data['url'])}\")\n",
    "\n",
    "  img_path = os.path.join(TEMP_FOLDER, os.path.basename(image_data['url']))\n",
    "  img_filename = os.path.splitext(os.path.basename(image_data['url']))[0]\n",
    "  img_data = get_image_info(img_path)\n",
    "\n",
    "  # Create the iiif_prezi3.Manifest:\n",
    "  manifest = arvesttools.manifest_creation.media_to_manifest(img_path)\n",
    "\n",
    "  # Update the ID to be the online location of the image:\n",
    "  manifest.items[0].items[0].items[0].body.id = image_data['url']\n",
    "\n",
    "  # Save the Manifest to disk\n",
    "  local_path = os.path.join(TEMP_FOLDER, f\"{slugify(img_filename)}.json\")\n",
    "  write_json(local_path, manifest.dict())\n",
    "\n",
    "  # And upload to Arvest:\n",
    "  added_manifest = ar.add_manifest(path = local_path, update_id = True)\n",
    "  added_manifest.update_title(f\"{img_filename}\")\n",
    "  added_manifest.update_description(\"Local view of an image embedding projection.\")\n",
    "  \n",
    "  manifest_metadata = added_manifest.get_metadata()\n",
    "  manifest_metadata[\"creator\"] = \"Image embedding projection tutorial\"\n",
    "  manifest_metadata[\"identifier\"] = \"&&API-TUTORIAL-IMAGE-EMBEDDING\"\n",
    "  added_manifest.update_metadata(manifest_metadata)\n",
    "\n",
    "  # Keep track of the urls that are created:\n",
    "  MANIFEST_DICT[image_data['url']] = added_manifest.get_full_url()\n",
    "\n",
    "print(\"üëç Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create the main visualization Manifest. First, we need to upload the image we created of the projection to Arvest. For this, we'll use the `add_media()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_media = ar.add_media(path = IMAGE_PATH)\n",
    "added_media.update_title(\"Image collection projection\")\n",
    "added_media.update_description(\"A projection in 2D space of a collection of images.\")\n",
    "\n",
    "media_metadata = added_media.get_metadata()\n",
    "media_metadata[\"creator\"] = \"Image embedding projection tutorial\"\n",
    "media_metadata[\"identifier\"] = \"&&API-TUTORIAL-IMAGE-EMBEDDING\"\n",
    "added_media.update_metadata(media_metadata)\n",
    "\n",
    "print(f\"üëç Media uploaded to Arvest at the following url: {added_media.get_full_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the Manifest. Again, we'll use the `media_to_manifest()` function, which in this case can also accept an Arvest media item. \n",
    "\n",
    "Once we've created the Manifest, we can add annotations to render it interactive. We'll add an annotation for each of the Manifests  created earlier using the `add_textual_annotation()` function with the corresponding Manifest url and spatial position and dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Manifest:\n",
    "manifest = arvesttools.manifest_creation.media_to_manifest(added_media)\n",
    "\n",
    "# Add an annotation for each Manifest:\n",
    "for item in coordinates[\"images\"]:\n",
    "    image_url = item[\"url\"]\n",
    "    manifest_url = MANIFEST_DICT[image_url]\n",
    "    xywh = {\"x\" : item[\"x\"], \"y\" : item[\"y\"], \"w\" : item[\"w\"], \"h\" : item[\"h\"]}\n",
    "    \n",
    "    arvesttools.manifest_creation.add_textual_annotation(\n",
    "        manifest,\n",
    "        text_content = f\"<p>{os.path.basename(image_url)}</p>\",\n",
    "        xywh = xywh,\n",
    "        linked_manifest = manifest_url\n",
    "    )\n",
    "\n",
    "# Save to disk:\n",
    "local_path = os.path.join(TEMP_FOLDER, \"projection-manifest.json\")\n",
    "write_json(local_path, manifest.dict())\n",
    "\n",
    "# And upload to Arvest:\n",
    "added_manifest = ar.add_manifest(path = local_path, update_id = True)\n",
    "added_manifest.update_title(\"Image embedding projection\")\n",
    "added_manifest.update_description(\"Projection of a collection of images in 2-D space.\")\n",
    "\n",
    "manifest_metadata = added_manifest.get_metadata()\n",
    "manifest_metadata[\"creator\"] = \"Image embedding projection tutorial\"\n",
    "manifest_metadata[\"identifier\"] = \"&&API-TUTORIAL-IMAGE-EMBEDDING\"\n",
    "added_manifest.update_metadata(manifest_metadata)\n",
    "\n",
    "print(f\"üëç Manifest uploaded to Arvest at the following url: {added_manifest.get_preview_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cleanup\n",
    "To finish, lets clean up our mess! First, we can delete the temporary folder ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TEMP_FOLDER)\n",
    "os.remove(IMAGE_PATH)\n",
    "os.remove(COORDINATES_PATH)\n",
    "print(f\"üóëÔ∏è {TEMP_FOLDER} removed !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can remove from Arvest all of our content. We can get all of our content by using the `get_manifests()` and `get_medias()` functions, then check the metadata. If it's one of the files we want to remove, we can then use the `remove()` function.\n",
    "\n",
    "**‚ö†Ô∏è Warning: there's no going back after using the remove function, so be careful! To avoid accidential removal, we've added a `REMOVE` variable that need to be set to `True` for the code to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE = True\n",
    "\n",
    "if REMOVE:\n",
    "    all_manifests = ar.get_manifests()\n",
    "    count = 0\n",
    "    print(\"Removing manifests...\")\n",
    "\n",
    "    for i, media_file in enumerate(all_manifests):\n",
    "        print_progress_bar_colab(i + 1, len(all_manifests), f\"(Processing file {i + 1}/{len(all_manifests)})\")\n",
    "        media_metadata = media_file.get_metadata()\n",
    "        if media_metadata[\"creator\"] == \"Image embedding projection tutorial\" and media_metadata[\"identifier\"] == \"&&API-TUTORIAL-IMAGE-EMBEDDING\":\n",
    "            media_file.remove()\n",
    "            count = count + 1\n",
    "\n",
    "    all_media = ar.get_medias()\n",
    "    print(\"Removing medias...\")\n",
    "\n",
    "    for i, media_file in enumerate(all_media):\n",
    "        print_progress_bar_colab(i + 1, len(all_media), f\"(Processing file {i + 1}/{len(all_media)})\")\n",
    "        media_metadata = media_file.get_metadata()\n",
    "        if media_metadata[\"creator\"] == \"Image embedding projection tutorial\" and media_metadata[\"identifier\"] == \"&&API-TUTORIAL-IMAGE-EMBEDDING\":\n",
    "            media_file.remove()\n",
    "            count = count + 1\n",
    "\n",
    "    print(f\"üóëÔ∏è Removed {count} items!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
