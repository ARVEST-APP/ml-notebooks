{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![video decomposition clustering](https://raw.githubusercontent.com/ARVEST-APP/ml-notebooks/refs/heads/main/docs/images/notebooks/video-decomposiiton-clustering.png)\n",
    "\n",
    "In this notebook, we shall start learning the [Arvest](https://arvest.app/en) API and its [python package](https://github.com/ARVEST-APP/arvest-api) by taking some videos that we have stored online, decomposing them into images, and using k-means clustering to create some interactive interfaces that we can upload and use in Arvest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "Let's begin by installing and importing all of the different components we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing and importing packages...\n",
      "üëç Ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"Installing and importing packages...\")\n",
    "\n",
    "# Uninstall and reinstall packages for a clean environment\n",
    "!pip uninstall -q -y arvestapi\n",
    "!pip uninstall -q -y arvesttools\n",
    "!pip uninstall -q -y jhutils\n",
    "!pip uninstall -q -y iiif_prezi3\n",
    "!pip uninstall -q -y dvt\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/ARVEST-APP/arvest-api.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/ARVEST-APP/arvest-api-tools.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/jdchart/jh-py-utils.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/iiif-prezi/iiif-prezi3.git\n",
    "!pip install -q --disable-pip-version-check git+https://github.com/distant-viewing/dvt.git\n",
    "!pip install -q --disable-pip-version-check opencv-python\n",
    "!pip install -q --disable-pip-version-check scikit-learn\n",
    "!pip install -q --disable-pip-version-check matplotlib\n",
    "\n",
    "# Import packages\n",
    "import arvestapi\n",
    "import arvesttools.manifest_creation\n",
    "from jhutils.local_files import read_json, write_json, get_image_info, collect_files\n",
    "import jhutils.online_files\n",
    "from jhutils.misc import print_progress_bar, slugify\n",
    "import os\n",
    "import dvt\n",
    "import iiif_prezi3\n",
    "import shutil\n",
    "import requests\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import mimetypes\n",
    "\n",
    "TEMP_FOLDER = os.path.join(os.getcwd(), \"_TEMP\")\n",
    "if os.path.isdir(TEMP_FOLDER) == False:\n",
    "    os.makedirs(TEMP_FOLDER)\n",
    "\n",
    "print(\"üëç Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you're following a workshop session...\n",
    "\n",
    "If you're currently in a workshop - hello! üëã\n",
    "\n",
    "You can run this cell to download all of the data into your colab session so that you don't have to run all of the time-consuming processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_IN_A_WORKSHOP = False\n",
    "\n",
    "if IM_IN_A_WORKSHOP:\n",
    "    # Download and unpack zip:\n",
    "    jhutils.online_files.download_zip(\"\", os.path.join(os.getcwd(), \"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Arvest\n",
    "\n",
    "First, we need to \"connect\" to Arvest using the Arvest API package. For this, we need our user email and our password which we will give to an instance of the `arvestapi.Arvest()` class. For convenience, we've saved ours in a file which is why we get `LOGIN_DATA` by reading a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç Succesfully connected to Arvest with \"Jacob\"\n"
     ]
    }
   ],
   "source": [
    "# First, let's connect to our Arvest account:\n",
    "LOGIN_DATA = os.path.join(os.getcwd(), \"login_private.json\")\n",
    "credentials = read_json(LOGIN_DATA)\n",
    "\n",
    "ar = arvestapi.Arvest(credentials[\"email\"], credentials[\"password\"])\n",
    "print(f\"üëç Succesfully connected to Arvest with \\\"{ar.profile.name}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get videos\n",
    "First we will need some sources to process! In this example, we shall be comparing different productions of Pina Bausch's [_Caf√© M√ºller_](https://en.wikipedia.org/wiki/Caf%C3%A9_M%C3%BCller). We shall be comparing three videos which are found at the following URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_URLS = {\n",
    "    \"video_1\" : \"https://youtu.be/ONtu1t0h1gQ?si=yfKV38rF5RAmxW7L\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the videos to Arvest:\n",
    "\n",
    "We can first use the Arvest API to add these videos to our Arvest account using the `add_media()` function. Let's do that now!\n",
    "\n",
    "The `add_media()` function returns a representation of the object in Arvest, which will allow us to modify things like it's `title`, `description` and `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_name in VIDEO_URLS:\n",
    "    \n",
    "    # Create the media item in Arvest:\n",
    "    added_media = ar.add_media(path = VIDEO_URLS[video_name])\n",
    "\n",
    "    # Update the media item:\n",
    "    added_media.update_title(video_name)\n",
    "    added_media.update_description(\"A performance of Pina Bausch's Caf√© M√ºller\")\n",
    "\n",
    "    # We can also update the item's metadata:\n",
    "    item_metadata = added_media.get_metadata()\n",
    "    item_metadata[\"identifier\"] = f\"&&WORKSHOP-API-CONTENT-{video_name}\"\n",
    "    added_media.update_metadata(item_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the videos for processing\n",
    "\n",
    "In order to process the videos we will need to be able to access them locally, which means that we will need to download them into our session. \n",
    "\n",
    "‚ÑπÔ∏è This step will be skipped if you downloaded the workshop elements, however you cans till run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_VIDEO_PATHS = {}\n",
    "\n",
    "if not IM_IN_A_WORKSHOP:\n",
    "    for video_name in VIDEO_URLS:\n",
    "        path = jhutils.online_files.download(VIDEO_URLS[video_name], dir = os.path.join(os.getcwd(), \"data\", \"videos\"))\n",
    "        LOCAL_VIDEO_PATHS[video_name] = path\n",
    "else:\n",
    "    LOCAL_VIDEO_PATHS = {} # ADD THIS\n",
    "\n",
    "print(f\"üëç Downloaded videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract images\n",
    "\n",
    "We're going to process one image for every 5 seconds of video in order to get a good idea about the visual composition of each video. To do this we'll first need to extract images from the videos.\n",
    "\n",
    "‚ÑπÔ∏è This step will be skipped if you downloaded the workshop elements, however you cans till run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% Complete (treating video_1)...\n",
      "üèûÔ∏è Finished extracting images!\n"
     ]
    }
   ],
   "source": [
    "LOCAL_IMAGE_PATHS = {}\n",
    "INTERVAL = 1\n",
    "\n",
    "if not IM_IN_A_WORKSHOP:\n",
    "    for i, video_name in enumerate(LOCAL_VIDEO_PATHS):\n",
    "        print_progress_bar(i + 1, len(LOCAL_VIDEO_PATHS), f\"(treating {video_name})...\")\n",
    "\n",
    "        # Output folder:\n",
    "        out_folder = os.path.join(os.getcwd(), \"data\", \"images\", video_name)\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "        LOCAL_IMAGE_PATHS[video_name] = out_folder\n",
    "\n",
    "        # Open video file with opencv 2 and get properties\n",
    "        cap = cv2.VideoCapture(LOCAL_VIDEO_PATHS[video_name])\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = total_frames / fps\n",
    "\n",
    "        # For iteration:\n",
    "        frame_interval = int(fps * INTERVAL)\n",
    "        frame_num = 0\n",
    "        saved_frame_count = 0\n",
    "\n",
    "        while frame_num < total_frames:\n",
    "            # Get frame\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Save image\n",
    "            output_path = os.path.join(out_folder, f\"frame_{saved_frame_count:04d}.jpg\")\n",
    "            cv2.imwrite(output_path, frame)\n",
    "\n",
    "            saved_frame_count += 1\n",
    "            frame_num += frame_interval\n",
    "\n",
    "        cap.release()\n",
    "else:\n",
    "    LOCAL_IMAGE_PATHS = {} # ADD THIS\n",
    "\n",
    "print(\"üèûÔ∏è Finished extracting images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Process embeddings\n",
    "Next we can use the distant viewing toolkit to map the images within an embedding space. The first time you use dvt it will download the model onto your computer. We'll save the embedding data as a **numpy file** (`.npy`) so that we don't have to run this step again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% Complete Treating frame_0031.jpg\n",
      "[[-0.03379547 -0.21241432 -0.11296684 ...  0.24943791 -0.13283801\n",
      "   0.02285932]\n",
      " [ 0.11780826 -0.12091831 -0.14023873 ...  0.12563564 -0.1249859\n",
      "  -0.01937349]\n",
      " [ 0.38491026 -0.08492123 -0.05013636 ...  1.1693214  -0.04501805\n",
      "   0.2564414 ]\n",
      " ...\n",
      " [ 0.09809799 -0.11525958 -0.11219667 ...  0.08334374 -0.10009596\n",
      "   0.02966112]\n",
      " [-0.12422692 -0.14223471 -0.13878013 ...  0.45014828 -0.11285977\n",
      "   0.17033623]\n",
      " [-0.070604   -0.1521217  -0.15332255 ...  0.45926207 -0.07086968\n",
      "   0.06665158]]\n"
     ]
    }
   ],
   "source": [
    "for i, video_name in enumerate(LOCAL_IMAGE_PATHS):\n",
    "    corpus = collect_files(LOCAL_IMAGE_PATHS[video_name], [\"jpg\"])\n",
    "    embedding_file = os.path.join(LOCAL_IMAGE_PATHS[video_name], \"_embeddings.npy\")\n",
    "\n",
    "    # Instance of dvt AnnoEmbed class:\n",
    "    embedder = dvt.AnnoEmbed()\n",
    "\n",
    "    for i, image_file in enumerate(corpus):\n",
    "        print_progress_bar(i + 1, len(corpus), f\"Treating {os.path.basename(image_file)}\")\n",
    "\n",
    "        image_as_np = cv2.imread(image_file)\n",
    "        image_as_np = cv2.cvtColor(image_as_np, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        embedding = embedder.run(image_as_np)[\"embedding\"]\n",
    "        if i == 0:\n",
    "            embedding_list = embedding\n",
    "        else:\n",
    "            embedding_list = np.vstack((embedding_list, embedding))\n",
    "\n",
    "    print(embedding_list)\n",
    "    np.save(embedding_file, embedding_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dimensionality reduction and clustering\n",
    "Now that we have our embedding data, we can use dimensionality reduction to crunch all of these dimensions down into 2 so that they can be projected into a 2-dimensional space. To do this, we'll use an one of the following dimensionality reduction algorithms: [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding), [PCA]() or [UMAP](). Note that we also do some pre- and post-processing, the full process is: standardisation -> dimensionality reduction -> normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "normalized = []\n",
    "\n",
    "for i, video_name in enumerate(LOCAL_IMAGE_PATHS):\n",
    "    corpus = collect_files(LOCAL_IMAGE_PATHS[video_name], [\"jpg\"])\n",
    "    embedding_file = os.path.join(LOCAL_IMAGE_PATHS[video_name], \"_embeddings.npy\")\n",
    "\n",
    "    embedding_list = np.load(embedding_file)\n",
    "    standardized = StandardScaler().fit_transform(embedding_list)\n",
    "\n",
    "    tsne = TSNE(n_components = 2, perplexity = 50, learning_rate=200, n_iter=5000)\n",
    "    reduced = tsne.fit_transform(standardized)\n",
    "\n",
    "    normalized.append(MinMaxScaler((0, 1)).fit_transform(reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we like, we can visualize the data in a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_DISPLAY = 0\n",
    "\n",
    "transposed = np.transpose(normalized[TO_DISPLAY])\n",
    "plt.scatter(transposed[0], transposed[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (optional)\n",
    "\n",
    "Next we could perform some clustering on this data using [K-Means](https://en.wikipedia.org/wiki/K-means_clustering). We won't be using this data for our visualisation, but it is something that could potentially be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 6\n",
    "\n",
    "kmeans = KMeans(n_clusters = NUM_CLUSTERS, random_state = 0, n_init = \"auto\")\n",
    "clusters = kmeans.fit(normalized).labels_\n",
    "\n",
    "# Create a random colour map for visualisation:\n",
    "colour_map = {}\n",
    "used = []\n",
    "for item in clusters:\n",
    "    if item not in used:\n",
    "        colour_map[str(item)] = (random.random(), random.random(), random.random())\n",
    "        used.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the clusters in a scatter plot like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed = np.transpose(normalized)\n",
    "col = []\n",
    "for item in clusters:\n",
    "    col.append(colour_map[str(item)])\n",
    "\n",
    "plt.scatter(transposed[0], transposed[1], c = col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Export to Arvest\n",
    "Finally, we shall export the results of our analysis to an image file, and create an annotated (and therefore interactive) IIIF Manifest that can be consulted in [Arvest](https://arvest.app/en). First, we shall create a high-res PNG file that projects the corresponding images into the 2D space of the dimensionality reduction. We shall also keep a track of the coordinates so that we can create our annotations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = os.path.join(os.getcwd(), \"visualization-image.png\")\n",
    "COORDINATES_PATH = os.path.join(os.getcwd(), \"visualization-coordinates.json\")\n",
    "\n",
    "WIDTH = 5000\n",
    "HEIGHT = 5000\n",
    "PADDING = 100\n",
    "IMAGE_ZOOM = 0.1\n",
    "\n",
    "def scale(val, old_min, old_max, new_min, new_max):\n",
    "    return new_min + (((val - old_min) * (new_max - new_min)) / (old_max - old_min))\n",
    "\n",
    "# Function for adding each image to the main image:\n",
    "def add_image(full_image, coordinates_list, image_url, coordinates):\n",
    "  img_path = os.path.join(TEMP_FOLDER, os.path.basename(image_url))\n",
    "  img_data = get_image_info(img_path)\n",
    "  this_img = Image.open(img_path)\n",
    "\n",
    "  w = int(img_data[\"width\"] * IMAGE_ZOOM)\n",
    "  h = int(img_data[\"height\"] * IMAGE_ZOOM)\n",
    "  x = int(scale(int(int(float(coordinates[0]) * WIDTH) - (w * 0.5)), 0, WIDTH, PADDING, WIDTH - (PADDING * 2)))\n",
    "  y = int(scale(int(int(float(coordinates[1]) * HEIGHT) - (h * 0.5)), 0, HEIGHT, PADDING, HEIGHT - (PADDING * 2)))\n",
    "\n",
    "  this_img = this_img.resize((w, h))\n",
    "  full_image.paste(this_img, (x, y))\n",
    "\n",
    "  coordinates_list.append({\"url\" : image_url, \"x\" : x, \"y\" : y, \"w\" : w, \"h\" : h})\n",
    "\n",
    "# Initialize image and coordinates\n",
    "full_image = Image.new('RGBA', (WIDTH, HEIGHT))\n",
    "coordinates = {\"images\" : []}\n",
    "\n",
    "# Add all of the images:\n",
    "for i, item in enumerate(normalized):\n",
    "  image_data = corpus[i]\n",
    "  print_progress_bar(i, len(corpus) - 1, f\"Treating {os.path.basename(image_data['url'])}\")\n",
    "  add_image(full_image, coordinates[\"images\"], image_data['url'], item)\n",
    "\n",
    "full_image.save(IMAGE_PATH)\n",
    "write_json(COORDINATES_PATH, coordinates)\n",
    "\n",
    "print(\"üé® Image created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Manifests\n",
    "Now we need to create our Manifests. In order to make the main visualization Manifest truly interactive, we shall also make a _Manifest for each of the images in our corpus_. This must be done first, as we will need the URLs of these Manifests when creating our annotations.\n",
    "\n",
    "First, we need to \"connect\" to Arvest using the Arvest API package. For this, we need our user email and our password which we will give to an instance of the `arvestapi.Arvest()` class. For convenience, we've saved ours in a file which is why we get `LOGIN_DATA` by reading a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's connect to our Arvest account:\n",
    "LOGIN_DATA = os.path.join(os.getcwd(), \"login_private.json\")\n",
    "credentials = read_json(LOGIN_DATA)\n",
    "\n",
    "ar = arvestapi.Arvest(credentials[\"email\"], credentials[\"password\"])\n",
    "print(f\"üëç Succesfully connected to Arvest with \\\"{ar.profile.name}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our Manifests using the [arvesttools](https://github.com/ARVEST-APP/arvest-api-tools) package's helper function `media_to_manifest()`. We'll create a Manifest for each file in our corpus, and keep a track of the URLs which are created in the `MANIFEST_DICT` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANIFEST_DICT = {}\n",
    "\n",
    "for i, image_data in enumerate(corpus):\n",
    "  print_progress_bar(i + 1, len(corpus), f\"Creating a Manifest for {os.path.basename(image_data['url'])}\")\n",
    "\n",
    "  img_path = os.path.join(TEMP_FOLDER, os.path.basename(image_data['url']))\n",
    "  img_filename = os.path.splitext(os.path.basename(image_data['url']))[0]\n",
    "  img_data = get_image_info(img_path)\n",
    "\n",
    "  # Create the iiif_prezi3.Manifest:\n",
    "  manifest = arvesttools.manifest_creation.media_to_manifest(img_path)\n",
    "\n",
    "  # Update the ID to be the online location of the image:\n",
    "  manifest.items[0].items[0].items[0].body.id = image_data['url']\n",
    "\n",
    "  # Save the Manifest to disk\n",
    "  local_path = os.path.join(TEMP_FOLDER, f\"{slugify(img_filename)}.json\")\n",
    "  write_json(local_path, manifest.dict())\n",
    "\n",
    "  # And upload to Arvest:\n",
    "  added_manifest = ar.add_manifest(path = local_path, update_id = True)\n",
    "  added_manifest.update_title(f\"{img_filename}\")\n",
    "  added_manifest.update_description(\"Local view of an image embedding projection.\")\n",
    "  \n",
    "  manifest_metadata = added_manifest.get_metadata()\n",
    "  manifest_metadata[\"creator\"] = \"Image embedding projection tutorial\"\n",
    "  manifest_metadata[\"identifier\"] = \"&&API-TUTORIAL-IMAGE-EMBEDDING\"\n",
    "  added_manifest.update_metadata(manifest_metadata)\n",
    "\n",
    "  # Keep track of the urls that are created:\n",
    "  MANIFEST_DICT[image_data['url']] = added_manifest.get_full_url()\n",
    "\n",
    "print(\"üëç Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create the main visualization Manifest. First, we need to upload the image we created of the projection to Arvest. For this, we'll use the `add_media()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_media = ar.add_media(path = IMAGE_PATH)\n",
    "added_media.update_title(\"Image collection projection\")\n",
    "added_media.update_description(\"A projection in 2D space of a collection of images.\")\n",
    "\n",
    "media_metadata = added_media.get_metadata()\n",
    "media_metadata[\"creator\"] = \"Image embedding projection tutorial\"\n",
    "media_metadata[\"identifier\"] = \"&&API-TUTORIAL-IMAGE-EMBEDDING\"\n",
    "added_media.update_metadata(media_metadata)\n",
    "\n",
    "print(f\"üëç Media uploaded to Arvest at the following url: {added_media.get_full_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the Manifest. Again, we'll use the `media_to_manifest()` function, which in this case can also accept an Arvest media item. \n",
    "\n",
    "Once we've created the Manifest, we can add annotations to render it interactive. We'll add an annotation for each of the Manifests  created earlier using the `add_textual_annotation()` function with the corresponding Manifest url and spatial position and dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Manifest:\n",
    "manifest = arvesttools.manifest_creation.media_to_manifest(added_media)\n",
    "\n",
    "# Add an annotation for each Manifest:\n",
    "for item in coordinates[\"images\"]:\n",
    "    image_url = item[\"url\"]\n",
    "    manifest_url = MANIFEST_DICT[image_url]\n",
    "    xywh = {\"x\" : item[\"x\"], \"y\" : item[\"y\"], \"w\" : item[\"w\"], \"h\" : item[\"h\"]}\n",
    "    \n",
    "    arvesttools.manifest_creation.add_textual_annotation(\n",
    "        manifest,\n",
    "        text_content = f\"<p>{os.path.basename(image_url)}</p>\",\n",
    "        xywh = xywh,\n",
    "        linked_manifest = manifest_url\n",
    "    )\n",
    "\n",
    "# Save to disk:\n",
    "local_path = os.path.join(TEMP_FOLDER, \"projection-manifest.json\")\n",
    "write_json(local_path, manifest.dict())\n",
    "\n",
    "# And upload to Arvest:\n",
    "added_manifest = ar.add_manifest(path = local_path, update_id = True)\n",
    "added_manifest.update_title(\"Image embedding projection\")\n",
    "added_manifest.update_description(\"Projection of a collection of images in 2-D space.\")\n",
    "\n",
    "manifest_metadata = added_manifest.get_metadata()\n",
    "manifest_metadata[\"creator\"] = \"Image embedding projection tutorial\"\n",
    "manifest_metadata[\"identifier\"] = \"&&API-TUTORIAL-IMAGE-EMBEDDING\"\n",
    "added_manifest.update_metadata(manifest_metadata)\n",
    "\n",
    "print(f\"üëç Manifest uploaded to Arvest at the following url: {added_manifest.get_preview_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cleanup\n",
    "To finish, lets clean up our mess! First, we can delete the temporary folder ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TEMP_FOLDER)\n",
    "os.remove(IMAGE_PATH)\n",
    "os.remove(COORDINATES_PATH)\n",
    "print(f\"üóëÔ∏è {TEMP_FOLDER} removed !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can remove from Arvest all of our content. We can get all of our content by using the `get_manifests()` and `get_medias()` functions, then check the metadata. If it's one of the files we want to remove, we can then use the `remove()` function.\n",
    "\n",
    "**‚ö†Ô∏è Warning: there's no going back after using the remove function, so be careful! To avoid accidential removal, we've added a `REMOVE` variable that need to be set to `True` for the code to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE = True\n",
    "\n",
    "if REMOVE:\n",
    "    all_manifests = ar.get_manifests()\n",
    "    count = 0\n",
    "    print(\"Removing manifests...\")\n",
    "\n",
    "    for i, media_file in enumerate(all_manifests):\n",
    "        print_progress_bar(i + 1, len(all_manifests), f\"(Processing file {i + 1}/{len(all_manifests)})\")\n",
    "        media_metadata = media_file.get_metadata()\n",
    "        if media_metadata[\"creator\"] == \"Image embedding projection tutorial\" and media_metadata[\"identifier\"] == \"&&API-TUTORIAL-IMAGE-EMBEDDING\":\n",
    "            media_file.remove()\n",
    "            count = count + 1\n",
    "\n",
    "    all_media = ar.get_medias()\n",
    "    print(\"Removing medias...\")\n",
    "\n",
    "    for i, media_file in enumerate(all_media):\n",
    "        print_progress_bar(i + 1, len(all_media), f\"(Processing file {i + 1}/{len(all_media)})\")\n",
    "        media_metadata = media_file.get_metadata()\n",
    "        if media_metadata[\"creator\"] == \"Image embedding projection tutorial\" and media_metadata[\"identifier\"] == \"&&API-TUTORIAL-IMAGE-EMBEDDING\":\n",
    "            media_file.remove()\n",
    "            count = count + 1\n",
    "\n",
    "    print(f\"üóëÔ∏è Removed {count} items!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
